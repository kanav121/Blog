[
  {
    "objectID": "posts/Job-Search/index.html",
    "href": "posts/Job-Search/index.html",
    "title": "How I got multiple offers and Landed My Dream Job in Fintech",
    "section": "",
    "text": "Are you tired of feeling stuck in your career? Do you dream of making a significant change but don’t know where to start? As a data scientist specializing in time series modeling and regression-based models using R, I felt exactly the same way. But I’m here to tell you that with the right mindset and a little bit of hard work, anything is possible."
  },
  {
    "objectID": "posts/Job-Search/index.html#introduction-my-career-shift-story-and-how-you-can-do-it-too",
    "href": "posts/Job-Search/index.html#introduction-my-career-shift-story-and-how-you-can-do-it-too",
    "title": "How I got multiple offers and Landed My Dream Job in Fintech",
    "section": "Introduction: My career shift story and how you can do it too",
    "text": "Introduction: My career shift story and how you can do it too\nI made the decision to shift my career from the energy sector to either health or finance industries, with a goal of securing a job that would offer at least a 120% salary increase from my current position, ideally in the FinTech space. And guess what? I not only achieved that goal but also landed my dream job!\nDon’t settle for a career that doesn’t excite you. Take charge of your future and make the change you’ve been dreaming of. Join me on this journey, and let’s make 2023 the year of success and fulfillment!\nIn this blog post, I’ll share my journey with you and provide valuable insights and tips for how you too can make a career shift and achieve your dream job. From studying Python and machine learning basics to creating resumes and profiles on job portals, I’ll guide you through the entire process. I’ll also share my experience with the roller-coaster of interviews and how I overcame my lack of knowledge in machine learning algorithms."
  },
  {
    "objectID": "posts/Job-Search/index.html#initial-plan-and-challenges",
    "href": "posts/Job-Search/index.html#initial-plan-and-challenges",
    "title": "How I got multiple offers and Landed My Dream Job in Fintech",
    "section": "Initial plan and challenges",
    "text": "Initial plan and challenges\nBack in September, I had planned to study Python and machine learning basics for a month before creating an online profile. Unfortunately, my motivation dwindled and I spent two months halfheartedly reading articles and making notes out of them. Nonetheless, I eventually created a resume using resources like codebasics and Krish Naik’s videos and established profiles on job portals such as Naukri and LinkedIn. Soon after, I started receiving calls from various organizations, but the interview process was a reality check for me as my understanding of machine learning algorithms was only surface-level."
  },
  {
    "objectID": "posts/Job-Search/index.html#learning-from-failures",
    "href": "posts/Job-Search/index.html#learning-from-failures",
    "title": "How I got multiple offers and Landed My Dream Job in Fintech",
    "section": "Learning from failures",
    "text": "Learning from failures\nIn November, I learned some valuable lessons through my experiences.I would usually clear first round but would fail in second round one such example was I confidently answered questions related to my resume but was stumped when asked to elaborate on categorical data encoding like difference between label and hot encoding. Similarly, when asked about hypothesis testing, I gave a confused response and said alpha was 0.5 instead of 0.05. Despite making notes on Krish Naik’s Live Statistics playlist, I hadn’t covered these topics sincerely.\nHowever, instead of giving up, I used these experiences as motivation to learn and grow. I went back to my notes, re-studied the topics where I had made mistakes, and updated my notes accordingly. I realized that learning is a continuous process and that we can always improve.\nIn the introduction, it is mentioned that I have specialized in time series data. However, due to the limited availability of data for some clients, I did not have much exposure to seasonal data and had only relied on regression-based models for time series analysis. Although I had previously attempted to use models such as Prophet and ARIMA, they did not yield favorable results, and I had not included them in my resume nor prepared them for the interviews. Unfortunately, during a technical interview(3rd technical round), I was asked about ACF, PACF, and the ADF test, and I was unable to answer these questions. To address this knowledge gap, I dedicated the next few weeks to learning the basics of time series terminology and models.\n\nGaining confidence\nDespite a dry spell in December where I didn’t receive any recruiter calls, I didn’t lose hope. Instead, I gained the confidence needed to crack technical interviews. I made sure to prepare well, studying topics in which I was lacking."
  },
  {
    "objectID": "posts/Job-Search/index.html#silver-lining",
    "href": "posts/Job-Search/index.html#silver-lining",
    "title": "How I got multiple offers and Landed My Dream Job in Fintech",
    "section": "Silver Lining",
    "text": "Silver Lining\nLong story short in January, I had received two offers with a 50% salary increase, but they were not in the FinTech industry, which was my goal. I was satisfied but not happy. Then, I cleared the first round of interviews for two companies, X and Y, on the same day while having a high fever (I’m not blowing my own trumpet, I mentioned this to make the point that if you work hard you will gain confidence eventually). One of the interviewers suggested I should reschedule, but I declined as I did not want to tarnish my reputation."
  },
  {
    "objectID": "posts/Job-Search/index.html#alls-well-that-ends-well",
    "href": "posts/Job-Search/index.html#alls-well-that-ends-well",
    "title": "How I got multiple offers and Landed My Dream Job in Fintech",
    "section": "All’s Well that Ends Well",
    "text": "All’s Well that Ends Well\nIn February, I received an offer letter from company X with a 95% salary increase. During my last round of interviews with company Y, I showed them this offer, and they offered me a 150% salary increase, which was a dream come true. I deleted my profile from Naukri and this company Y happened to be TransUnion, a FinTech company where I joined as a Senior Analyst."
  },
  {
    "objectID": "posts/Job-Search/index.html#the-reality-of-recruitment-process",
    "href": "posts/Job-Search/index.html#the-reality-of-recruitment-process",
    "title": "How I got multiple offers and Landed My Dream Job in Fintech",
    "section": "The Reality of Recruitment Process",
    "text": "The Reality of Recruitment Process\nI realized early on that the recruitment process is not always straightforward. I cleared the first round in one organization, only to be informed weeks later that they had filled the position internally. In two other organizations, after clearing the first round, I was later told that openings were closed. In another organization, I cleared three rounds and even submitted my documents, only to be told later that I was the second priority and the position was filled via referral. However, I didn’t let these setbacks discourage me, and I continued to persevere."
  },
  {
    "objectID": "posts/Job-Search/index.html#the-journey-continues",
    "href": "posts/Job-Search/index.html#the-journey-continues",
    "title": "How I got multiple offers and Landed My Dream Job in Fintech",
    "section": "The Journey Continues",
    "text": "The Journey Continues\nI learned that mistakes are an integral part of the learning process. Even though I had an offer from TransUnion I gave another interview (just of fun). The interview taught me about SARIMAX (we can use this for multivariate time series) and I knew only about ARIMA,SARIMA etc. which are univariate models. It was a humbling experience, but it taught me a valuable lesson that ‘You may think you know but actually you don’t’."
  },
  {
    "objectID": "posts/Job-Search/index.html#insights",
    "href": "posts/Job-Search/index.html#insights",
    "title": "How I got multiple offers and Landed My Dream Job in Fintech",
    "section": "Insights",
    "text": "Insights\n1. Set a clear and measurable goal for yourself.\n2. I found that building a strong online profile is essential when looking for job opportunities even before you started preparing because it take time to get a call.\n3. Make your resume plain and simple. Tailor your resume and cover letter to each position. Highlight your relevant skills and experience and show how you can add value to the company.\n4. Prepare for interviews by researching the company and practicing your responses to common interview questions. Dress professionally and arrive on time.\n5. It’s better to have a deep understanding of a few machine learning algorithms rather than a superficial understanding of many.\n6. Interviews can be nerve-wracking, but it’s essential to remain calm and not let anxiety take over.\n7. Keep your notes updated and review them regularly\n8. Follow up after interviews with a thank-you email or note, and express your continued interest in the position and also ask for the feed back(very important).\n9. There is no such thing called as perfect interview, you can’t answer 100% of the questions.\n10. Don’t be afraid to negotiate job offers. Do your research on salary ranges for the position and make a counter offer if necessary. This is the most important tip I can give you, as it can make a huge difference in your income and satisfaction."
  },
  {
    "objectID": "posts/Job-Search/index.html#study-sources",
    "href": "posts/Job-Search/index.html#study-sources",
    "title": "How I got multiple offers and Landed My Dream Job in Fintech",
    "section": "Study Sources",
    "text": "Study Sources\nI found the book “Approaching Almost Any Machine Learning Problem” by Abhishek Thakur to be an excellent resource for building a strong foundation in machine learning.\nI also relied heavily on online resources like Statquest and Krish Naik’s videos, as well as blogs on various websites like Medium, Towards Data Science, and Analytics Vidhya, like for Decision tree I think KDnuggets have one of the finest blog on it.\nMachine Learning Interviews Book by Chip Huyen was another source from where I learned non technical (like how to negotiate salary) & technical aspect of interview."
  },
  {
    "objectID": "posts/Job-Search/index.html#conclusion",
    "href": "posts/Job-Search/index.html#conclusion",
    "title": "How I got multiple offers and Landed My Dream Job in Fintech",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, my journey to landing a Senior Analyst role in a FinTech company was not easy, but it taught me valuable lessons about perseverance, learning from mistakes, and the importance of building a strong online profile. I hope my journey can inspire others to pursue their dreams and never give up, no matter how challenging the journey may be.\nHope that’s the key, be willing to work hard ‘you will get success may not be today but tomorrow you surely will’."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "FastAI Course Lecture 2 Notes\n\n\n\nComputer Vision\n\n\nFastAI\n\n\n\n\n\n\n\nKanav Sharma\n\n\nApr 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I got multiple offers and Landed My Dream Job in Fintech\n\n\n\nInformative\n\n\nInterview\n\n\n\nA blog regarding how I got what I wanted\n\n\n\nKanav Sharma\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my digital space! I’m a data scientist in FinTech venturing into the exciting world of Deep Learning. Over the coming months, I’ll share my data science projects and learnings here as I delve deeper into AI and its boundless potential.\nWhether you’re a fellow data enthusiast, an AI aficionado, or simply curious, join me on this captivating journey. Get ready to explore the magic of data, algorithms, and the frontier of Deep Learning. Let’s unravel mysteries and unlock new frontiers together! I’m always eager to learn, so if you’d like to provide feedback, guidance, or engage in discussion on any topic or project, feel free to reach out to me at kanav608@gmail.com."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nSGVU, Jaipur | 2015-2019\nBtech - CSE | 7.96 CGPA"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nTransunion | Senior Analyst | March 2023 - Present\nKreate Energy Pvt Ltd | Junior Associate | Feb 2020 - March 2023\nRoboMq Pvt Ltd | Data Scientist | January 2019 - January 2020"
  },
  {
    "objectID": "posts/FastAI_Course_Lect2/index.html",
    "href": "posts/FastAI_Course_Lect2/index.html",
    "title": "FastAI Course Lecture 2 Notes",
    "section": "",
    "text": "Let’s install all required packages\n#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n#hide\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m719.8/719.8 kB[0m [31m7.6 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m510.5/510.5 kB[0m [31m10.6 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m116.3/116.3 kB[0m [31m12.1 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m194.1/194.1 kB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m134.8/134.8 kB[0m [31m15.4 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.6/1.6 MB[0m [31m16.3 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m23.7/23.7 MB[0m [31m15.0 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m823.6/823.6 kB[0m [31m26.3 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m14.1/14.1 MB[0m [31m53.7 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m731.7/731.7 MB[0m [31m901.8 kB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m410.6/410.6 MB[0m [31m2.0 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m121.6/121.6 MB[0m [31m8.2 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m56.5/56.5 MB[0m [31m12.1 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m124.2/124.2 MB[0m [31m8.3 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m196.0/196.0 MB[0m [31m2.3 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m166.0/166.0 MB[0m [31m7.2 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m99.1/99.1 kB[0m [31m15.6 MB/s[0m eta [36m0:00:00[0m\n[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m21.1/21.1 MB[0m [31m68.6 MB/s[0m eta [36m0:00:00[0m\n[?25hMounted at /content/gdrive\n\n\nExtract Data using DuckDuckGo function 1. Create dynamic path according to there name store file 2. Create a dictionary to track the number of downloaded images per category (e.g., cat).\n??search_images_ddg\nSignature: search_images_ddg(term, max_images=200)\ncat_types = 'Leopard','Cougar','Tiger','Lion','Cheetah','SnowLeopard'\npath = Path('CAT')\n\n#remove folder with file in it\nimport shutil\nif path.exists():\n  shutil.rmtree(path)\n\nper_cat_count = {}\n\nif not path.exists():\n    path.mkdir()\n    for o in cat_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        results = search_images_ddg(f'{o}')\n        download_images(dest, urls=results)\n        per_cat_count[f'{o}'] = len(results)\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nCount of Images per category\nper_cat_count\n{'Leopard': 200,\n 'Cougar': 200,\n 'Tiger': 200,\n 'Lion': 200,\n 'Cheetah': 200,\n 'SnowLeopard': 200}\nSo we got 200 images per type\nWhile downloading we can get corrupt images lets first remove them. verify_images() will return path of such images and using unlink we can remove these files.\nfns = get_image_files(path)\ntotal_imagelength = len(fns)\nfailed = verify_images(fns)\nfailed_imagelength = len(failed)\nfailed\n(#51) [Path('CAT/Lion/d61427d6-f097-4727-a3ba-de31366199d6.jpg'),Path('CAT/Lion/85f17699-5ebe-4e88-9798-14b7e66281d7.png'),Path('CAT/Lion/84589c8d-a1da-45be-9fe6-1f87a34289b3.jpg'),Path('CAT/Lion/cf746926-23d6-4754-b7d0-25779410ee15.jpg'),Path('CAT/SnowLeopard/f3ce804b-5071-4312-8633-9895e721340c.jpg'),Path('CAT/SnowLeopard/961333aa-79da-4dc2-9f56-f4d07697a14e.jpg'),Path('CAT/SnowLeopard/57b3a667-1d3e-47bb-8fff-3e1505a5a12f.jpg'),Path('CAT/SnowLeopard/1b3f2639-1c5d-4e26-86c8-feb4b99bf76a.jpg'),Path('CAT/Cougar/cd7a89c9-8667-4d24-aceb-3a85bb7b247e.jpg'),Path('CAT/Cougar/44b6067c-f159-4d5f-819a-11e7d50a3fd8.jpg')...]\nfailed.map(Path.unlink);\nDict = {\"Total_Image_Count\": total_imagelength, \"Failed_Image_Count\": failed_imagelength}\nDict\n{'Total_Image_Count': 1115, 'Failed_Image_Count': 51}\n\n\n\nCreate a data block and load that data block in data loader.\n\nData Block - Is a blueprint on how to assemble data that we want to send for training.\nData Loader - Is used to pass that data which is in batch format(i.e created using data blocks) to the GPU.\n\nbig_cat = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\ndls = big_cat.dataloaders(path)\nDataBlock is a convenient way to organize the data loading, splitting, and transformation steps in preparation for training a deep learning model using the fastai library.\nDataBlock(): is suitable for a classification task where you have a dataset of images, and each image belongs to a specific category (e.g., types of cats).\nblocks=(ImageBlock, CategoryBlock): It specify that our input are images & our target are categories(types of cat)\nget_image_files: this help to get list of all the images from subfolder.\nparent_label: This is a function that extracts the labels (categories) for each item.’Leapord’,‘Tiger’,‘Lion’\nA DataLoaders includes validation and training DataLoader. Let’s check random validation dataset.\ndls.valid.show_batch(max_n=6, nrows=2)\n\n\n\npng\n\n\nSquishing or Padding for Model Training :\n\nSquishing or padding is applied to images during training.\nCropping may result in data loss, while squishing/stretching can lead to unrealistic shapes, impacting accuracy.\nPadding may introduce excessive empty space, causing wasted computation.\n\nPractical Approach - Data Augmentation: The idea of getting different picture every time from same image is called data augmentation.\n\nRandomly select and crop parts of the image during each epoch.\nTrain the model on different image parts across multiple epochs.\nThis approach creates random variations in input data without altering its meaning.\nAiming to provide diverse perspectives, it ensures the model sees different pictures from the same image in each iteration..\n\n\nTo train our model, we’ll use RandomResizedCrop with an image size of 224 px, which is fairly standard for image classification, and default aug_transforms:\nbig_cat = big_cat.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\nbig_cat_dls = big_cat.dataloaders(path)\nbig_cat_dls.train.show_batch(max_n=8, nrows=2)\n\n\n\npng\n\n\n\n\n\nTip1 - Prioritaize to train a quick and simple model first, rather than going for big model directly.\nTip2 - Build model first and then clean the data. And then again train the model.\nlearn = vision_learner(big_cat_dls, resnet18, metrics=error_rate)\nlearn.fine_tune(8)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.878671\n\n\n0.548061\n\n\n0.183962\n\n\n00:36\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.669399\n\n\n0.442041\n\n\n0.160377\n\n\n00:36\n\n\n\n\n1\n\n\n0.566393\n\n\n0.425440\n\n\n0.136792\n\n\n00:38\n\n\n\n\n2\n\n\n0.476826\n\n\n0.555463\n\n\n0.179245\n\n\n00:38\n\n\n\n\n3\n\n\n0.429597\n\n\n0.524273\n\n\n0.146226\n\n\n00:37\n\n\n\n\n4\n\n\n0.367606\n\n\n0.519690\n\n\n0.117925\n\n\n00:36\n\n\n\n\n5\n\n\n0.319734\n\n\n0.529199\n\n\n0.113208\n\n\n00:37\n\n\n\n\n6\n\n\n0.287094\n\n\n0.516044\n\n\n0.127358\n\n\n00:38\n\n\n\n\n7\n\n\n0.260760\n\n\n0.514551\n\n\n0.132075\n\n\n00:36\n\n\n\n\n\nHere we see, in last epoch rise in error_rate which means that in stochastic gradient descent we have surpassed deepest point and trending towards upward direction which leads to higher loss rate. It indicates that the training process should likely be stopped to prevent further divergence from the optimal solution\n###Visualize Confusion Matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\npng\n\n\nplot_top_losses shows us the images with the highest loss in our dataset.\ninterp.plot_top_losses(4, nrows=1, figsize=(18,4))\n\n\n\n\npng\n\n\n\n\n\nImageClassifierCleaner enables us to review all images associated with a specific category and identify their placement within the dataloader, whether in the training or validation set.\nThe images are organized in ascending order of confidence, prioritizing those with the highest loss. This allows for efficient data sorting by simply examining the initial images. Users can choose to keep, delete, or modify the category label (type of cat) as needed.\n#hide_output\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\nVBox(children=(Dropdown(options=('Cheetah', 'Cougar', 'Leopard', 'Lion', 'SnowLeopard', 'Tiger'), value='Cheet…\nThe Cleaner possesses information regarding the files we deleted and whose labels we modified. Now, we will implement these changes.\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\n\nbig_cat = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\nbig_cat = big_cat.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\nbig_cat_dls = big_cat.dataloaders(path)\nbig_cat_dls.train.show_batch(max_n=8, nrows=2)\n\n\n\n\npng\n\n\nlearn = vision_learner(big_cat_dls, resnet34, metrics=error_rate)\nlearn.fine_tune(8)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00&lt;00:00, 121MB/s]\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.839337\n\n\n0.557398\n\n\n0.161137\n\n\n00:36\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.552628\n\n\n0.516515\n\n\n0.137441\n\n\n00:37\n\n\n\n\n1\n\n\n0.457381\n\n\n0.544474\n\n\n0.175355\n\n\n00:42\n\n\n\n\n2\n\n\n0.399777\n\n\n0.615449\n\n\n0.146919\n\n\n00:38\n\n\n\n\n3\n\n\n0.345620\n\n\n0.601597\n\n\n0.151659\n\n\n00:40\n\n\n\n\n4\n\n\n0.293677\n\n\n0.630620\n\n\n0.146919\n\n\n00:37\n\n\n\n\n5\n\n\n0.256501\n\n\n0.669779\n\n\n0.137441\n\n\n00:37\n\n\n\n\n6\n\n\n0.227690\n\n\n0.648144\n\n\n0.142180\n\n\n00:36\n\n\n\n\n7\n\n\n0.207254\n\n\n0.651927\n\n\n0.137441\n\n\n00:37\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\npng\n\n\ninterp.plot_top_losses(4, nrows=1, figsize=(17,4))\n\n\n\n\npng\n\n\n\n\n\n\n\n\nlearn.export('Lecture2_Big_Cat_Model.pkl')\n\n\n\nYou can access live model here deployed using Hugging Face & gradio. Wanna know how to do it ? refer Gradio-HuggingFace.\nYou can access repo here"
  },
  {
    "objectID": "posts/FastAI_Course_Lect2/index.html#download-and-sort-out-the-dataimages.",
    "href": "posts/FastAI_Course_Lect2/index.html#download-and-sort-out-the-dataimages.",
    "title": "FastAI Course Lecture 2 Notes",
    "section": "",
    "text": "Extract Data using DuckDuckGo function 1. Create dynamic path according to there name store file 2. Create a dictionary to track the number of downloaded images per category (e.g., cat).\n??search_images_ddg\nSignature: search_images_ddg(term, max_images=200)\ncat_types = 'Leopard','Cougar','Tiger','Lion','Cheetah','SnowLeopard'\npath = Path('CAT')\n\n#remove folder with file in it\nimport shutil\nif path.exists():\n  shutil.rmtree(path)\n\nper_cat_count = {}\n\nif not path.exists():\n    path.mkdir()\n    for o in cat_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        results = search_images_ddg(f'{o}')\n        download_images(dest, urls=results)\n        per_cat_count[f'{o}'] = len(results)\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nCount of Images per category\nper_cat_count\n{'Leopard': 200,\n 'Cougar': 200,\n 'Tiger': 200,\n 'Lion': 200,\n 'Cheetah': 200,\n 'SnowLeopard': 200}\nSo we got 200 images per type\nWhile downloading we can get corrupt images lets first remove them. verify_images() will return path of such images and using unlink we can remove these files.\nfns = get_image_files(path)\ntotal_imagelength = len(fns)\nfailed = verify_images(fns)\nfailed_imagelength = len(failed)\nfailed\n(#51) [Path('CAT/Lion/d61427d6-f097-4727-a3ba-de31366199d6.jpg'),Path('CAT/Lion/85f17699-5ebe-4e88-9798-14b7e66281d7.png'),Path('CAT/Lion/84589c8d-a1da-45be-9fe6-1f87a34289b3.jpg'),Path('CAT/Lion/cf746926-23d6-4754-b7d0-25779410ee15.jpg'),Path('CAT/SnowLeopard/f3ce804b-5071-4312-8633-9895e721340c.jpg'),Path('CAT/SnowLeopard/961333aa-79da-4dc2-9f56-f4d07697a14e.jpg'),Path('CAT/SnowLeopard/57b3a667-1d3e-47bb-8fff-3e1505a5a12f.jpg'),Path('CAT/SnowLeopard/1b3f2639-1c5d-4e26-86c8-feb4b99bf76a.jpg'),Path('CAT/Cougar/cd7a89c9-8667-4d24-aceb-3a85bb7b247e.jpg'),Path('CAT/Cougar/44b6067c-f159-4d5f-819a-11e7d50a3fd8.jpg')...]\nfailed.map(Path.unlink);\nDict = {\"Total_Image_Count\": total_imagelength, \"Failed_Image_Count\": failed_imagelength}\nDict\n{'Total_Image_Count': 1115, 'Failed_Image_Count': 51}"
  },
  {
    "objectID": "posts/FastAI_Course_Lect2/index.html#prepare-data-for-model-training-data-loaders-data-augmentaion-etc..",
    "href": "posts/FastAI_Course_Lect2/index.html#prepare-data-for-model-training-data-loaders-data-augmentaion-etc..",
    "title": "FastAI Course Lecture 2 Notes",
    "section": "",
    "text": "Create a data block and load that data block in data loader.\n\nData Block - Is a blueprint on how to assemble data that we want to send for training.\nData Loader - Is used to pass that data which is in batch format(i.e created using data blocks) to the GPU.\n\nbig_cat = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\ndls = big_cat.dataloaders(path)\nDataBlock is a convenient way to organize the data loading, splitting, and transformation steps in preparation for training a deep learning model using the fastai library.\nDataBlock(): is suitable for a classification task where you have a dataset of images, and each image belongs to a specific category (e.g., types of cats).\nblocks=(ImageBlock, CategoryBlock): It specify that our input are images & our target are categories(types of cat)\nget_image_files: this help to get list of all the images from subfolder.\nparent_label: This is a function that extracts the labels (categories) for each item.’Leapord’,‘Tiger’,‘Lion’\nA DataLoaders includes validation and training DataLoader. Let’s check random validation dataset.\ndls.valid.show_batch(max_n=6, nrows=2)\n\n\n\npng\n\n\nSquishing or Padding for Model Training :\n\nSquishing or padding is applied to images during training.\nCropping may result in data loss, while squishing/stretching can lead to unrealistic shapes, impacting accuracy.\nPadding may introduce excessive empty space, causing wasted computation.\n\nPractical Approach - Data Augmentation: The idea of getting different picture every time from same image is called data augmentation.\n\nRandomly select and crop parts of the image during each epoch.\nTrain the model on different image parts across multiple epochs.\nThis approach creates random variations in input data without altering its meaning.\nAiming to provide diverse perspectives, it ensures the model sees different pictures from the same image in each iteration..\n\n\nTo train our model, we’ll use RandomResizedCrop with an image size of 224 px, which is fairly standard for image classification, and default aug_transforms:\nbig_cat = big_cat.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\nbig_cat_dls = big_cat.dataloaders(path)\nbig_cat_dls.train.show_batch(max_n=8, nrows=2)\n\n\n\npng"
  },
  {
    "objectID": "posts/FastAI_Course_Lect2/index.html#train-the-model",
    "href": "posts/FastAI_Course_Lect2/index.html#train-the-model",
    "title": "FastAI Course Lecture 2 Notes",
    "section": "",
    "text": "Tip1 - Prioritaize to train a quick and simple model first, rather than going for big model directly.\nTip2 - Build model first and then clean the data. And then again train the model.\nlearn = vision_learner(big_cat_dls, resnet18, metrics=error_rate)\nlearn.fine_tune(8)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.878671\n\n\n0.548061\n\n\n0.183962\n\n\n00:36\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.669399\n\n\n0.442041\n\n\n0.160377\n\n\n00:36\n\n\n\n\n1\n\n\n0.566393\n\n\n0.425440\n\n\n0.136792\n\n\n00:38\n\n\n\n\n2\n\n\n0.476826\n\n\n0.555463\n\n\n0.179245\n\n\n00:38\n\n\n\n\n3\n\n\n0.429597\n\n\n0.524273\n\n\n0.146226\n\n\n00:37\n\n\n\n\n4\n\n\n0.367606\n\n\n0.519690\n\n\n0.117925\n\n\n00:36\n\n\n\n\n5\n\n\n0.319734\n\n\n0.529199\n\n\n0.113208\n\n\n00:37\n\n\n\n\n6\n\n\n0.287094\n\n\n0.516044\n\n\n0.127358\n\n\n00:38\n\n\n\n\n7\n\n\n0.260760\n\n\n0.514551\n\n\n0.132075\n\n\n00:36\n\n\n\n\n\nHere we see, in last epoch rise in error_rate which means that in stochastic gradient descent we have surpassed deepest point and trending towards upward direction which leads to higher loss rate. It indicates that the training process should likely be stopped to prevent further divergence from the optimal solution\n###Visualize Confusion Matrix\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\npng\n\n\nplot_top_losses shows us the images with the highest loss in our dataset.\ninterp.plot_top_losses(4, nrows=1, figsize=(18,4))\n\n\n\n\npng"
  },
  {
    "objectID": "posts/FastAI_Course_Lect2/index.html#clean-the-data",
    "href": "posts/FastAI_Course_Lect2/index.html#clean-the-data",
    "title": "FastAI Course Lecture 2 Notes",
    "section": "",
    "text": "ImageClassifierCleaner enables us to review all images associated with a specific category and identify their placement within the dataloader, whether in the training or validation set.\nThe images are organized in ascending order of confidence, prioritizing those with the highest loss. This allows for efficient data sorting by simply examining the initial images. Users can choose to keep, delete, or modify the category label (type of cat) as needed.\n#hide_output\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\nVBox(children=(Dropdown(options=('Cheetah', 'Cougar', 'Leopard', 'Lion', 'SnowLeopard', 'Tiger'), value='Cheet…\nThe Cleaner possesses information regarding the files we deleted and whose labels we modified. Now, we will implement these changes.\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)"
  },
  {
    "objectID": "posts/FastAI_Course_Lect2/index.html#re-train-the-model-using-updated-data",
    "href": "posts/FastAI_Course_Lect2/index.html#re-train-the-model-using-updated-data",
    "title": "FastAI Course Lecture 2 Notes",
    "section": "",
    "text": "big_cat = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\nbig_cat = big_cat.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\nbig_cat_dls = big_cat.dataloaders(path)\nbig_cat_dls.train.show_batch(max_n=8, nrows=2)\n\n\n\n\npng\n\n\nlearn = vision_learner(big_cat_dls, resnet34, metrics=error_rate)\nlearn.fine_tune(8)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00&lt;00:00, 121MB/s]\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.839337\n\n\n0.557398\n\n\n0.161137\n\n\n00:36\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.552628\n\n\n0.516515\n\n\n0.137441\n\n\n00:37\n\n\n\n\n1\n\n\n0.457381\n\n\n0.544474\n\n\n0.175355\n\n\n00:42\n\n\n\n\n2\n\n\n0.399777\n\n\n0.615449\n\n\n0.146919\n\n\n00:38\n\n\n\n\n3\n\n\n0.345620\n\n\n0.601597\n\n\n0.151659\n\n\n00:40\n\n\n\n\n4\n\n\n0.293677\n\n\n0.630620\n\n\n0.146919\n\n\n00:37\n\n\n\n\n5\n\n\n0.256501\n\n\n0.669779\n\n\n0.137441\n\n\n00:37\n\n\n\n\n6\n\n\n0.227690\n\n\n0.648144\n\n\n0.142180\n\n\n00:36\n\n\n\n\n7\n\n\n0.207254\n\n\n0.651927\n\n\n0.137441\n\n\n00:37\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\npng\n\n\ninterp.plot_top_losses(4, nrows=1, figsize=(17,4))\n\n\n\n\npng"
  },
  {
    "objectID": "posts/FastAI_Course_Lect2/index.html#lets-download-our-model-and-use-it-make-prediction.-in-next-lesson-.",
    "href": "posts/FastAI_Course_Lect2/index.html#lets-download-our-model-and-use-it-make-prediction.-in-next-lesson-.",
    "title": "FastAI Course Lecture 2 Notes",
    "section": "",
    "text": "learn.export('Lecture2_Big_Cat_Model.pkl')"
  },
  {
    "objectID": "posts/FastAI_Course_Lect2/index.html#live-model",
    "href": "posts/FastAI_Course_Lect2/index.html#live-model",
    "title": "FastAI Course Lecture 2 Notes",
    "section": "",
    "text": "You can access live model here deployed using Hugging Face & gradio. Wanna know how to do it ? refer Gradio-HuggingFace.\nYou can access repo here"
  }
]